"""Common utility functions and constants for steerable_motion tasks."""

import json
import math
import os
import shutil
import subprocess
import tempfile
import time
import traceback
import uuid
from pathlib import Path
from datetime import datetime
from typing import Any, Generator

import cv2 # pip install opencv-python
import mediapipe as mp # pip install mediapipe
import numpy as np # pip install numpy
from PIL import Image, ImageDraw, ImageFont, ImageEnhance # pip install Pillow, ensure ImageEnhance is imported
import requests # Added for downloads
from urllib.parse import urlparse # Added for URL parsing
import urllib.parse

# --- Global Debug Mode ---
# This will be set by the main script (steerable_motion.py)
DEBUG_MODE = False

# --- Constants for DB interaction and defaults ---
STATUS_QUEUED = "Queued"
STATUS_IN_PROGRESS = "In Progress"
STATUS_COMPLETE = "Complete"
STATUS_FAILED = "Failed"
DEFAULT_DB_TABLE_NAME = "tasks"
# DEFAULT_MODEL_NAME = "vace_14B" # Defined in steerable_motion.py's argparser
# DEFAULT_SEGMENT_FRAMES = 81    # Defined in steerable_motion.py's argparser
# DEFAULT_FPS_HELPERS = 25       # Defined in steerable_motion.py's argparser
# DEFAULT_SEED = 12345           # Defined in steerable_motion.py's argparser

# --- Debug / Verbose Logging Helper ---
def dprint(msg: str):
    """Print a debug message if DEBUG_MODE is enabled."""
    if DEBUG_MODE:
        print(f"[DEBUG SM-COMMON {datetime.utcnow().isoformat()}Z] {msg}")

def extract_orchestrator_parameters(db_task_params: dict, task_id: str = "unknown", dprint=None) -> dict:
    """
    Centralized extraction of parameters from orchestrator_details.
    
    This handles the common pattern where task parameters contain nested orchestrator_details
    that need to be extracted and flattened into the main parameter space.
    
    Args:
        db_task_params: Raw task parameters from database
        task_id: Task ID for logging
        dprint: Optional debug print function
        
    Returns:
        Dict with extracted parameters added at the top level
    """
    extracted_params = db_task_params.copy()
    
    orchestrator_details = db_task_params.get("orchestrator_details", {})
    if orchestrator_details:
        if dprint:
            dprint(f"Task {task_id}: Found orchestrator_details with {len(orchestrator_details)} parameters")
        
        # Extract specific parameters that should be available at top level
        extraction_map = {
            "additional_loras": "additional_loras",
            "prompt": "prompt",
            "negative_prompt": "negative_prompt",
            "resolution": "resolution",
            "video_length": "video_length",
            "seed": "seed",
            "model": "model",
            "num_inference_steps": "num_inference_steps",
            "guidance_scale": "guidance_scale",
            "guidance2_scale": "guidance2_scale",
            "guidance3_scale": "guidance3_scale",
            "guidance_phases": "guidance_phases",
            "flow_shift": "flow_shift",
            "switch_threshold": "switch_threshold",
            "switch_threshold2": "switch_threshold2",
            "model_switch_phase": "model_switch_phase",
            "sample_solver": "sample_solver",
            # LoRA parameters from phase_config
            "lora_names": "lora_names",
            "lora_multipliers": "lora_multipliers",
            "activated_loras": "activated_loras",
            # Magic edit parameters
            "image_url": "image_url",
            "in_scene": "in_scene",
            # Qwen image style parameters
            "style_reference_image": "style_reference_image",
            "style_reference_strength": "style_reference_strength",
            # Additional common orchestrator parameters
            "video_guide": "video_guide",
            "video_mask": "video_mask",
            "video_prompt_type": "video_prompt_type",
            "control_net_weight": "control_net_weight",
            "amount_of_motion": "amount_of_motion",
            "phase_config": "phase_config",
        }
        
        extracted_count = 0
        for orchestrator_key, param_key in extraction_map.items():
            if orchestrator_key in orchestrator_details:
                # Only extract if not already present at top level (top level takes precedence)
                if param_key not in extracted_params:
                    value = orchestrator_details[orchestrator_key]
                    # For additional_loras, only extract if it has actual entries (not empty dict)
                    if orchestrator_key == "additional_loras" and not value:
                        continue
                    extracted_params[param_key] = value
                    extracted_count += 1
                    if dprint:
                        dprint(f"Task {task_id}: Extracted {orchestrator_key} from orchestrator_details")
        
        # Note: orchestrator_details is already in db_task_params, no need to duplicate
        
        if dprint and extracted_count > 0:
            dprint(f"Task {task_id}: Extracted {extracted_count} parameters from orchestrator_details")
    
    return extracted_params

# --- Helper Functions ---

def snap_resolution_to_model_grid(parsed_res: tuple[int, int]) -> tuple[int, int]:
    """
    Snaps resolution to model grid requirements (multiples of 16).
    
    Args:
        parsed_res: (width, height) tuple
        
    Returns:
        (width, height) tuple snapped to nearest valid values
    """
    width, height = parsed_res
    # Ensure resolution is compatible with model requirements (multiples of 16)
    width = (width // 16) * 16
    height = (height // 16) * 16
    return width, height

def ensure_valid_prompt(prompt: str | None) -> str:
    """
    Ensures prompt is valid (not None or empty), returns space as default.
    
    Args:
        prompt: Input prompt string or None
        
    Returns:
        Valid prompt string (space if input was None/empty)
    """
    if not prompt or not prompt.strip():
        return " "
    return prompt.strip()

def ensure_valid_negative_prompt(negative_prompt: str | None) -> str:
    """
    Ensures negative prompt is valid (not None), returns space as default.
    
    Args:
        negative_prompt: Input negative prompt string or None
        
    Returns:
        Valid negative prompt string (space if input was None/empty)
    """
    if not negative_prompt or not negative_prompt.strip():
        return " "
    return negative_prompt.strip()

def parse_resolution(res_str: str) -> tuple[int, int]:
    """Parses 'WIDTHxHEIGHT' string to (width, height) tuple."""
    try:
        w, h = map(int, res_str.split('x'))
        if w <= 0 or h <= 0:
            raise ValueError("Width and height must be positive.")
        return w, h
    except ValueError as e:
        raise ValueError(f"Resolution string must be in WIDTHxHEIGHT format with positive integers (e.g., '960x544'), got {res_str}. Error: {e}")

def generate_unique_task_id(prefix: str = "") -> str:
    """Generates a UUID4 string.

    The optional *prefix* parameter is now ignored so that the returned value
    is a bare RFC-4122 UUID which can be stored in a Postgres `uuid` column
    without casting errors.  The argument is kept in the signature to avoid
    breaking existing call-sites that still pass a prefix.
    """
    return str(uuid.uuid4())

def image_to_frame(image_path: str | Path, target_size: tuple[int, int]) -> np.ndarray | None:
    """Loads an image, resizes it, and converts to BGR NumPy array for OpenCV."""
    try:
        img = Image.open(image_path).convert("RGB")
        img = img.resize(target_size, Image.Resampling.LANCZOS)
        return cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
    except FileNotFoundError:
        print(f"Error: Image file not found at {image_path}")
        return None
    except Exception as e:
        print(f"Error loading or processing image {image_path}: {e}")
        return None

def create_color_frame(size: tuple[int, int], color_bgr: tuple[int, int, int] = (0, 0, 0)) -> np.ndarray:
    """Creates a single color BGR frame (default black)."""
    height, width = size[1], size[0] # size is (width, height)
    frame = np.full((height, width, 3), color_bgr, dtype=np.uint8)
    return frame

def get_easing_function(name: str):
    """
    Returns an easing function by name.
    """
    if name == 'linear':
        return lambda t: t
    elif name == 'ease_in_quad':
        return lambda t: t * t
    elif name == 'ease_out_quad':
        return lambda t: t * (2 - t)
    elif name == 'ease_in_out_quad' or name == 'ease_in_out': # Added alias
        return lambda t: 2 * t * t if t < 0.5 else -1 + (4 - 2 * t) * t
    elif name == 'ease_in_cubic':
        return lambda t: t * t * t
    elif name == 'ease_out_cubic':
        return lambda t: 1 + ((t - 1) ** 3)
    elif name == 'ease_in_out_cubic':
        return lambda t: 4 * t * t * t if t < 0.5 else 1 + ((2 * t - 2) ** 3) / 2
    elif name == 'ease_in_quart':
        return lambda t: t * t * t * t
    elif name == 'ease_out_quart':
        return lambda t: 1 - ((t - 1) ** 4)
    elif name == 'ease_in_out_quart':
        return lambda t: 8 * t * t * t * t if t < 0.5 else 1 - ((-2 * t + 2) ** 4) / 2
    elif name == 'ease_in_quint':
        return lambda t: t * t * t * t * t
    elif name == 'ease_out_quint':
        return lambda t: 1 + ((t - 1) ** 5)
    elif name == 'ease_in_out_quint':
        return lambda t: 16 * t * t * t * t * t if t < 0.5 else 1 + ((-2 * t + 2) ** 5) / 2
    elif name == 'ease_in_sine':
        return lambda t: 1 - math.cos(t * math.pi / 2)
    elif name == 'ease_out_sine':
        return lambda t: math.sin(t * math.pi / 2)
    elif name == 'ease_in_out_sine':
        return lambda t: -(math.cos(math.pi * t) - 1) / 2
    elif name == 'ease_in_expo':
        return lambda t: 0 if t == 0 else 2 ** (10 * (t - 1))
    elif name == 'ease_out_expo':
        return lambda t: 1 if t == 1 else 1 - 2 ** (-10 * t)
    elif name == 'ease_in_out_expo':
        if t == 0: return 0
        if t == 1: return 1
        if t < 0.5:
            return (2 ** (20 * t - 10)) / 2
        else:
            return (2 - 2 ** (-20 * t + 10)) / 2
    else: # Default to ease_in_out
        return lambda t: 2 * t * t if t < 0.5 else -1 + (4 - 2 * t) * t

def create_video_from_frames_list(
    frames_list: list[np.ndarray],
    output_path: str | Path,
    fps: int,
    resolution: tuple[int, int] # width, height
):
    """Creates an MP4 video from a list of NumPy BGR frames.
    
    This is a wrapper around video_utils.create_video_from_frames_list
    with BT.709 colorspace standardization enabled by default.
    """
    # Import here to avoid circular imports
    from .video_utils import create_video_from_frames_list as _create_video
    return _create_video(
        frames_list, 
        output_path, 
        fps, 
        resolution, 
        standardize_colorspace=True
    )

def extract_video_segment_ffmpeg(
    input_video_path: str | Path,
    output_video_path: str | Path,
    start_frame_index: int, # 0-indexed
    num_frames_to_keep: int,
    input_fps: float, # FPS of the input video for accurate -ss calculation
    resolution: tuple[int,int] 
):
    """Extracts a video segment using FFmpeg with stream copy if possible."""
    dprint(f"EXTRACT_VIDEO_SEGMENT_FFMPEG: Called with input='{input_video_path}', output='{output_video_path}', start_idx={start_frame_index}, num_frames={num_frames_to_keep}, input_fps={input_fps}")
    if num_frames_to_keep <= 0:
        print(f"Warning: num_frames_to_keep is {num_frames_to_keep} for {output_video_path} (FFmpeg). Nothing to extract.")
        dprint("EXTRACT_VIDEO_SEGMENT_FFMPEG: num_frames_to_keep is 0 or less, returning.")
        Path(output_video_path).unlink(missing_ok=True)
        return

    input_video_path = Path(input_video_path)
    output_video_path = Path(output_video_path)
    output_video_path.parent.mkdir(parents=True, exist_ok=True)

    start_time_seconds = start_frame_index / input_fps

    cmd = [
        'ffmpeg',
        '-y',  
        '-ss', str(start_time_seconds),
        '-i', str(input_video_path.resolve()),
        '-vframes', str(num_frames_to_keep),
        '-an', 
        str(output_video_path.resolve())
    ]

    dprint(f"EXTRACT_VIDEO_SEGMENT_FFMPEG: Running command: {' '.join(cmd)}")
    try:
        process = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8')
        dprint(f"EXTRACT_VIDEO_SEGMENT_FFMPEG: Successfully extracted segment to {output_video_path}")
        if process.stderr:
            dprint(f"FFmpeg stderr (for {output_video_path}):\n{process.stderr}")
        if not output_video_path.exists() or output_video_path.stat().st_size == 0:
            print(f"Error: FFmpeg command for {output_video_path} apparently succeeded but output file is missing or empty.")
            dprint(f"FFmpeg command for {output_video_path} produced no output. stdout:\n{process.stdout}\nstderr:\n{process.stderr}")

    except subprocess.CalledProcessError as e:
        print(f"Error during FFmpeg segment extraction for {output_video_path}:")
        print("FFmpeg command:", ' '.join(e.cmd))
        if e.stdout: print("FFmpeg stdout:\n", e.stdout)
        if e.stderr: print("FFmpeg stderr:\n", e.stderr)
        dprint(f"FFmpeg extraction failed for {output_video_path}. Error: {e}")
        Path(output_video_path).unlink(missing_ok=True)
    except FileNotFoundError:
        print("Error: ffmpeg command not found. Please ensure ffmpeg is installed and in your PATH.")
        dprint("FFmpeg command not found during segment extraction.")
        raise

def stitch_videos_ffmpeg(video_paths_list: list[str | Path], output_path: str | Path):
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    if not video_paths_list:
        print("No videos to stitch.")
        return

    valid_video_paths = []
    for p in video_paths_list:
        resolved_p = Path(p).resolve()
        if resolved_p.exists() and resolved_p.stat().st_size > 0:
            valid_video_paths.append(resolved_p)
        else:
            print(f"Warning: Video segment {resolved_p} is missing or empty. Skipping from stitch list.")
    
    if not valid_video_paths:
        print("No valid video segments found to stitch after checks.")
        return

    with tempfile.TemporaryDirectory(prefix="ffmpeg_concat_") as tmpdir:
        filelist_path = Path(tmpdir) / "ffmpeg_filelist.txt"
        with open(filelist_path, 'w', encoding='utf-8') as f:
            for video_path in valid_video_paths:
                f.write(f"file '{video_path.as_posix()}'\n")
        
        cmd = [
            'ffmpeg', '-y', '-f', 'concat', '-safe', '0',
            '-i', str(filelist_path),
            '-c', 'copy', str(output_path)
        ]
        
        print(f"Running ffmpeg to stitch videos: {' '.join(cmd)}")
        try:
            process = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8')
            print(f"Successfully stitched videos into: {output_path}")
            if process.stderr: print("FFmpeg log (stderr):\n", process.stderr)
        except subprocess.CalledProcessError as e:
            print(f"Error during ffmpeg stitching for {output_path}:")
            print("FFmpeg command:", ' '.join(e.cmd))
            if e.stdout: print("FFmpeg stdout:\n", e.stdout)
            if e.stderr: print("FFmpeg stderr:\n", e.stderr)
            raise 
        except FileNotFoundError:
            print("Error: ffmpeg command not found. Please ensure ffmpeg is installed and in your PATH.")
            raise

def save_frame_from_video(video_path: Path, frame_index: int, output_image_path: Path, resolution: tuple[int, int]):
    """Extracts a specific frame from a video, resizes, and saves it as an image."""
    dprint(f"SAVE_FRAME_FROM_VIDEO: Input='{video_path}', Index={frame_index}, Output='{output_image_path}', Res={resolution}")
    if not video_path.exists() or video_path.stat().st_size == 0:
        print(f"Error: Video file for frame extraction not found or empty: {video_path}")
        return False

    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        print(f"Error: Could not open video file: {video_path}")
        return False

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # Allow Python‚Äêstyle negative indexing (e.g. -1 for last frame)
    if frame_index < 0:
        frame_index = total_frames + frame_index  # Convert to positive index

    if frame_index < 0 or frame_index >= total_frames:
        print(f"Error: Frame index {frame_index} is out of bounds for video {video_path} (total frames: {total_frames}).")
        cap.release()
        return False

    cap.set(cv2.CAP_PROP_POS_FRAMES, float(frame_index))
    ret, frame = cap.read()
    cap.release()

    if not ret or frame is None:
        print(f"Error: Could not read frame {frame_index} from {video_path}.")
        return False

    try:
        if frame.shape[1] != resolution[0] or frame.shape[0] != resolution[1]:
            dprint(f"SAVE_FRAME_FROM_VIDEO: Resizing frame from {frame.shape[:2]} to {resolution[:2][::-1]}")
            frame = cv2.resize(frame, resolution, interpolation=cv2.INTER_AREA)
        
        output_image_path.parent.mkdir(parents=True, exist_ok=True)
        cv2.imwrite(str(output_image_path), frame)
        print(f"Successfully saved frame {frame_index} from {video_path} to {output_image_path}")
        return True
    except Exception as e:
        print(f"Error saving frame to {output_image_path}: {e}")
        traceback.print_exc()
        return False

# --- FFMPEG-based specific frame extraction ---
def extract_specific_frame_ffmpeg(
    input_video_path: str | Path,
    frame_number: int, # 0-indexed
    output_image_path: str | Path,
    input_fps: float # Passed by caller, though not strictly needed for ffmpeg frame index selection using 'eq(n,frame_number)'
):
    """Extracts a specific frame from a video using FFmpeg and saves it as an image."""
    dprint(f"EXTRACT_SPECIFIC_FRAME_FFMPEG: Input='{input_video_path}', Frame={frame_number}, Output='{output_image_path}'")
    input_video_p = Path(input_video_path)
    output_image_p = Path(output_image_path)
    output_image_p.parent.mkdir(parents=True, exist_ok=True)

    if not input_video_p.exists() or input_video_p.stat().st_size == 0:
        print(f"Error: Input video for frame extraction not found or empty: {input_video_p}")
        dprint(f"EXTRACT_SPECIFIC_FRAME_FFMPEG: Input video {input_video_p} not found or empty. Returning False.")
        return False

    cmd = [
        'ffmpeg',
        '-y',  # Overwrite output without asking
        '-i', str(input_video_p.resolve()),
        '-vf', f"select=eq(n\\,{frame_number})", # Escaped comma for ffmpeg filter syntax
        '-vframes', '1',
        str(output_image_p.resolve())
    ]

    dprint(f"EXTRACT_SPECIFIC_FRAME_FFMPEG: Running command: {' '.join(cmd)}")
    try:
        process = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8')
        dprint(f"EXTRACT_SPECIFIC_FRAME_FFMPEG: Successfully extracted frame {frame_number} to {output_image_p}")
        if process.stderr:
            dprint(f"FFmpeg stderr (for frame extraction to {output_image_p}):\n{process.stderr}")
        if not output_image_p.exists() or output_image_p.stat().st_size == 0:
            print(f"Error: FFmpeg command for frame extraction to {output_image_p} apparently succeeded but output file is missing or empty.")
            dprint(f"FFmpeg command for {output_image_p} (frame extraction) produced no output. stdout:\n{process.stdout}\nstderr:\n{process.stderr}")
            return False
        return True
    except subprocess.CalledProcessError as e:
        print(f"Error during FFmpeg frame extraction for {output_image_p}:")
        print("FFmpeg command:", ' '.join(e.cmd))
        if e.stdout: print("FFmpeg stdout:\n", e.stdout)
        if e.stderr: print("FFmpeg stderr:\n", e.stderr)
        dprint(f"FFmpeg frame extraction failed for {output_image_p}. Error: {e}")
        if output_image_p.exists(): output_image_p.unlink(missing_ok=True)
        return False
    except FileNotFoundError:
        print("Error: ffmpeg command not found. Please ensure ffmpeg is installed and in your PATH.")
        dprint("FFmpeg command not found during frame extraction.")
        raise

# --- FFMPEG-based video concatenation (alternative to stitch_videos_ffmpeg if caller manages temp dir) ---
def concatenate_videos_ffmpeg(
    video_paths: list[str | Path],
    output_path: str | Path,
    temp_dir_for_list: str | Path # Directory where the list file will be created
):
    """Concatenates multiple video files into one using FFmpeg, using a provided temp directory for the list file."""
    output_p = Path(output_path)
    output_p.parent.mkdir(parents=True, exist_ok=True)
    temp_dir_p = Path(temp_dir_for_list)
    temp_dir_p.mkdir(parents=True, exist_ok=True)

    if not video_paths:
        print("No videos to concatenate.")
        dprint("CONCATENATE_VIDEOS_FFMPEG: No video paths provided. Returning.")
        if output_p.exists(): output_p.unlink(missing_ok=True)
        return

    valid_video_paths = []
    for p_item in video_paths:
        resolved_p_item = Path(p_item).resolve()
        if resolved_p_item.exists() and resolved_p_item.stat().st_size > 0:
            valid_video_paths.append(resolved_p_item)
        else:
            print(f"Warning: Video segment {resolved_p_item} for concatenation is missing or empty. Skipping.")
            dprint(f"CONCATENATE_VIDEOS_FFMPEG: Skipping invalid video segment {resolved_p_item}")
    
    if not valid_video_paths:
        print("No valid video segments found to concatenate after checks.")
        dprint("CONCATENATE_VIDEOS_FFMPEG: No valid video segments. Returning.")
        if output_p.exists(): output_p.unlink(missing_ok=True)
        return

    filelist_path = temp_dir_p / "ffmpeg_concat_filelist.txt"
    with open(filelist_path, 'w', encoding='utf-8') as f:
        for video_path_item in valid_video_paths:
            f.write(f"file '{video_path_item.as_posix()}'\n") # Use as_posix() for ffmpeg list file
    
    cmd = [
        'ffmpeg', '-y', 
        '-f', 'concat', 
        '-safe', '0', 
        '-i', str(filelist_path.resolve()),
        '-c', 'copy', 
        str(output_p.resolve())
    ]
    
    dprint(f"CONCATENATE_VIDEOS_FFMPEG: Running command: {' '.join(cmd)} with list file {filelist_path}")
    try:
        process = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8')
        print(f"Successfully concatenated videos into: {output_p}")
        dprint(f"CONCATENATE_VIDEOS_FFMPEG: Success. Output: {output_p}")
        if process.stderr:
            dprint(f"FFmpeg stderr (for concatenation to {output_p}):\n{process.stderr}")
        if not output_p.exists() or output_p.stat().st_size == 0:
             print(f"Warning: FFmpeg concatenation to {output_p} apparently succeeded but output file is missing or empty.")
             dprint(f"FFmpeg command for {output_p} (concatenation) produced no output. stdout:\n{process.stdout}\nstderr:\n{process.stderr}")
    except subprocess.CalledProcessError as e:
        print(f"Error during FFmpeg concatenation for {output_p}:")
        print("FFmpeg command:", ' '.join(e.cmd))
        if e.stdout: print("FFmpeg stdout:\n", e.stdout)
        if e.stderr: print("FFmpeg stderr:\n", e.stderr)
        dprint(f"FFmpeg concatenation failed for {output_p}. Error: {e}")
        if output_p.exists(): output_p.unlink(missing_ok=True)
        raise 
    except FileNotFoundError:
        print("Error: ffmpeg command not found. Please ensure ffmpeg is installed and in your PATH.")
        dprint("CONCATENATE_VIDEOS_FFMPEG: ffmpeg command not found.")
        raise

# --- Video properties extraction with ffprobe fallback ---
def _get_video_props_ffprobe(video_path_str: str) -> tuple[int | None, float | None]:
    """
    Fallback method using ffprobe to get frame count and FPS.
    More reliable for WebM and other containers where OpenCV struggles.
    """
    import subprocess
    try:
        # Get frame count and fps using ffprobe
        # For containers without frame count in metadata, we use nb_read_frames (requires counting)
        # First try fast method: nb_frames from stream metadata
        probe_cmd = [
            'ffprobe', '-v', 'error',
            '-select_streams', 'v:0',
            '-show_entries', 'stream=nb_frames,r_frame_rate,duration',
            '-of', 'json',
            video_path_str
        ]
        result = subprocess.run(probe_cmd, capture_output=True, text=True, timeout=30)
        if result.returncode != 0:
            dprint(f"GET_VIDEO_PROPS_FFPROBE: ffprobe failed: {result.stderr}")
            return None, None
        
        import json
        data = json.loads(result.stdout)
        streams = data.get('streams', [])
        if not streams:
            return None, None
        
        stream = streams[0]
        
        # Parse FPS from r_frame_rate (e.g., "30/1" or "30000/1001")
        fps = None
        r_frame_rate = stream.get('r_frame_rate', '')
        if r_frame_rate and '/' in r_frame_rate:
            num, den = r_frame_rate.split('/')
            if float(den) > 0:
                fps = float(num) / float(den)
        
        # Try to get frame count from metadata
        frame_count = None
        nb_frames = stream.get('nb_frames')
        if nb_frames and nb_frames != 'N/A':
            try:
                frame_count = int(nb_frames)
            except (ValueError, TypeError):
                pass
        
        # If no frame count in metadata, estimate from duration * fps
        # This is common for WebM files
        if frame_count is None and fps and fps > 0:
            duration = stream.get('duration')
            if duration and duration != 'N/A':
                try:
                    frame_count = int(float(duration) * fps)
                    dprint(f"GET_VIDEO_PROPS_FFPROBE: Estimated frame count from duration: {frame_count}")
                except (ValueError, TypeError):
                    pass
        
        # Last resort: actually count frames (slow but accurate)
        if frame_count is None:
            dprint(f"GET_VIDEO_PROPS_FFPROBE: Counting frames for {video_path_str} (may be slow)...")
            count_cmd = [
                'ffprobe', '-v', 'error',
                '-select_streams', 'v:0',
                '-count_frames',
                '-show_entries', 'stream=nb_read_frames',
                '-of', 'csv=p=0',
                video_path_str
            ]
            count_result = subprocess.run(count_cmd, capture_output=True, text=True, timeout=120)
            if count_result.returncode == 0 and count_result.stdout.strip():
                try:
                    frame_count = int(count_result.stdout.strip())
                except (ValueError, TypeError):
                    pass
        
        dprint(f"GET_VIDEO_PROPS_FFPROBE: {video_path_str} - Frames: {frame_count}, FPS: {fps}")
        return frame_count, fps
        
    except subprocess.TimeoutExpired:
        dprint(f"GET_VIDEO_PROPS_FFPROBE: Timeout processing {video_path_str}")
        return None, None
    except Exception as e:
        dprint(f"GET_VIDEO_PROPS_FFPROBE: Exception processing {video_path_str}: {e}")
        return None, None


def get_video_frame_count_and_fps(video_path: str | Path) -> tuple[int | None, float | None]:
    """
    Gets frame count and FPS of a video.
    
    Uses OpenCV first (fast), then falls back to ffprobe for containers
    where OpenCV is unreliable (e.g., WebM with VP8/VP9 codec).
    
    Returns (None, None) on failure.
    """
    video_path_str = str(Path(video_path).resolve())
    cap = None
    try:
        cap = cv2.VideoCapture(video_path_str)
        if not cap.isOpened():
            dprint(f"GET_VIDEO_FRAME_COUNT_FPS: Could not open video with OpenCV: {video_path_str}")
            # Try ffprobe as fallback
            return _get_video_props_ffprobe(video_path_str)
        
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        # Validate frame_count and fps as they can sometimes be 0 or negative for problematic files/streams
        valid_frame_count = frame_count if frame_count > 0 else None
        valid_fps = fps if fps > 0 else None

        # If OpenCV failed to get frame count (common for WebM), try ffprobe
        if valid_frame_count is None:
            dprint(f"GET_VIDEO_FRAME_COUNT_FPS: OpenCV reported non-positive frame count ({frame_count}) for {video_path_str}. Trying ffprobe...")
            ffprobe_frame_count, ffprobe_fps = _get_video_props_ffprobe(video_path_str)
            if ffprobe_frame_count is not None:
                valid_frame_count = ffprobe_frame_count
            if valid_fps is None and ffprobe_fps is not None:
                valid_fps = ffprobe_fps
        
        if valid_fps is None:
            dprint(f"GET_VIDEO_FRAME_COUNT_FPS: Video {video_path_str} reported non-positive FPS: {fps}. Treating as unknown.")

        dprint(f"GET_VIDEO_FRAME_COUNT_FPS: Video {video_path_str} - Frames: {valid_frame_count}, FPS: {valid_fps}")
        return valid_frame_count, valid_fps
    except Exception as e:
        dprint(f"GET_VIDEO_FRAME_COUNT_FPS: Exception processing {video_path_str}: {e}")
        # Try ffprobe as fallback on any exception
        return _get_video_props_ffprobe(video_path_str)
    finally:
        if cap:
            cap.release()


body_colors = [
    [255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],
    [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],
    [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]
]
face_color = [255, 255, 255]
hand_keypoint_color = [0, 0, 255]
hand_limb_colors = [
    [255,0,0],[255,60,0],[255,120,0],[255,180,0], [180,255,0],[120,255,0],[60,255,0],[0,255,0],
    [0,255,60],[0,255,120],[0,255,180],[0,180,255], [0,120,255],[0,60,255],[0,0,255],[60,0,255],
    [120,0,255],[180,0,255],[255,0,180],[255,0,120]
]

# MediaPipe Pose connections (33 landmarks, indices 0-32)
# Based on mp.solutions.pose.POSE_CONNECTIONS
body_skeleton = [
    (0, 1), (1, 2), (2, 3), (3, 7),  # Nose to Left Eye to Left Ear
    (0, 4), (4, 5), (5, 6), (6, 8),  # Nose to Right Eye to Right Ear
    (9, 10),  # Mouth
    (11, 12), # Shoulders
    (11, 13), (13, 15), (15, 17), (17, 19), (15, 19), (15, 21), # Left Arm and simplified Left Hand (wrist to fingers)
    (12, 14), (14, 16), (16, 18), (18, 20), (16, 20), (16, 22), # Right Arm and simplified Right Hand (wrist to fingers)
    (11, 23), (12, 24), # Connect shoulders to Hips
    (23, 24), # Hip connection
    (23, 25), (25, 27), (27, 29), (29, 31), (27, 31), # Left Leg and Foot
    (24, 26), (26, 28), (28, 30), (30, 32), (28, 32)  # Right Leg and Foot
]

face_skeleton = [] # Draw face dots only, no connections

# MediaPipe Hand connections (21 landmarks per hand, indices 0-20)
hand_skeleton = [
    (0, 1), (1, 2), (2, 3), (3, 4),        # Thumb
    (0, 5), (5, 6), (6, 7), (7, 8),        # Index finger
    (0, 9), (9, 10), (10, 11), (11, 12),   # Middle finger
    (0, 13), (13, 14), (14, 15), (15, 16), # Ring finger
    (0, 17), (17, 18), (18, 19), (19, 20)  # Pinky finger
]

def draw_keypoints_and_skeleton(image, keypoints_data, skeleton_connections, colors_config, confidence_threshold=0.1, point_radius=3, line_thickness=2, is_face=False, is_hand=False):
    if not keypoints_data:
        return
    
    tri_tuples = []
    if isinstance(keypoints_data, list) and len(keypoints_data) > 0 and isinstance(keypoints_data[0], (int, float)) and len(keypoints_data) % 3 == 0:
        for i in range(0, len(keypoints_data), 3):
            tri_tuples.append(keypoints_data[i:i+3])
    else:
        dprint(f"draw_keypoints_and_skeleton: Unexpected keypoints_data format or length not divisible by 3. Data: {keypoints_data}")
        return

    if skeleton_connections:
        for i, (joint_idx_a, joint_idx_b) in enumerate(skeleton_connections):
            if joint_idx_a >= len(tri_tuples) or joint_idx_b >= len(tri_tuples):
                continue
            a_x, a_y, a_confidence = tri_tuples[joint_idx_a]
            b_x, b_y, b_confidence = tri_tuples[joint_idx_b]
            
            if a_confidence >= confidence_threshold and b_confidence >= confidence_threshold:
                limb_color = None
                if is_hand:
                    limb_color_list = colors_config['limbs']
                    limb_color = limb_color_list[i % len(limb_color_list)]
                else: 
                    limb_color_list = colors_config if isinstance(colors_config, list) else [colors_config]
                    limb_color = limb_color_list[i % len(limb_color_list)]
                if limb_color is not None:
                    cv2.line(image, (int(a_x), int(a_y)), (int(b_x), int(b_y)), limb_color, line_thickness)

    for i, (x, y, confidence) in enumerate(tri_tuples):
        if confidence >= confidence_threshold:
            point_color = None
            current_radius = point_radius
            if is_hand:
                point_color = colors_config['points']
            elif is_face:
                point_color = colors_config 
                current_radius = 2
            else: 
                point_color_list = colors_config 
                point_color = point_color_list[i % len(point_color_list)]
            if point_color is not None:
                cv2.circle(image, (int(x), int(y)), current_radius, point_color, -1)

def gen_skeleton_with_face_hands(pose_keypoints_2d, face_keypoints_2d, hand_left_keypoints_2d, hand_right_keypoints_2d,
                                 canvas_width, canvas_height, landmarkType, confidence_threshold=0.1):
    image = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)
    
    def scale_keypoints(keypoints, target_w, target_h, input_is_normalized):
        if not keypoints: return []
        scaled = []
        if not isinstance(keypoints, list) or (keypoints and not isinstance(keypoints[0], (int, float))):
            dprint(f"scale_keypoints: Unexpected keypoints format: {type(keypoints)}. Expecting flat list of numbers.")
            return [] 

        for i in range(0, len(keypoints), 3):
            x, y, conf = keypoints[i:i+3]
            if input_is_normalized: scaled.extend([x * target_w, y * target_h, conf])
            else: scaled.extend([x, y, conf])
        return scaled
    
    input_is_normalized = (landmarkType == "OpenPose") # This might need adjustment based on actual landmarkType usage
    
    scaled_pose = scale_keypoints(pose_keypoints_2d, canvas_width, canvas_height, input_is_normalized)
    scaled_face = scale_keypoints(face_keypoints_2d, canvas_width, canvas_height, input_is_normalized)
    scaled_hand_left = scale_keypoints(hand_left_keypoints_2d, canvas_width, canvas_height, input_is_normalized)
    scaled_hand_right = scale_keypoints(hand_right_keypoints_2d, canvas_width, canvas_height, input_is_normalized)
    
    draw_keypoints_and_skeleton(image, scaled_pose, body_skeleton, body_colors, confidence_threshold, point_radius=6, line_thickness=4)
    if scaled_face:
        draw_keypoints_and_skeleton(image, scaled_face, face_skeleton, face_color, confidence_threshold, point_radius=2, line_thickness=1, is_face=True)
    hand_colors_config = {'limbs': hand_limb_colors, 'points': hand_keypoint_color}
    if scaled_hand_left:
        draw_keypoints_and_skeleton(image, scaled_hand_left, hand_skeleton, hand_colors_config, confidence_threshold, point_radius=3, line_thickness=2, is_hand=True)
    if scaled_hand_right:
        draw_keypoints_and_skeleton(image, scaled_hand_right, hand_skeleton, hand_colors_config, confidence_threshold, point_radius=3, line_thickness=2, is_hand=True)
    return image

def transform_all_keypoints(keypoints_1_dict, keypoints_2_dict, frames, interpolation="linear"):
    def interpolate_keypoint_set(kp1_list, kp2_list, num_frames, interp_method):
        if not kp1_list and not kp2_list: return [[] for _ in range(num_frames)]
        
        len1 = len(kp1_list) if kp1_list else 0
        len2 = len(kp2_list) if kp2_list else 0

        if not kp1_list: kp1_list = [0.0] * len2
        if not kp2_list: kp2_list = [0.0] * len1

        if len(kp1_list) != len(kp2_list) or not kp1_list or len(kp1_list) % 3 != 0:
             dprint(f"interpolate_keypoint_set: Mismatched, empty, or non-triplet keypoint lists after padding. KP1 len: {len(kp1_list)}, KP2 len: {len(kp2_list)}. Returning empty sequences.")
             return [[] for _ in range(num_frames)]
        
        tri_tuples_1 = [kp1_list[i:i + 3] for i in range(0, len(kp1_list), 3)]
        tri_tuples_2 = [kp2_list[i:i + 3] for i in range(0, len(kp2_list), 3)]
        
        keypoints_sequence = []
        for j in range(num_frames):
            interpolated_kps_for_frame = []
            t = j / float(num_frames - 1) if num_frames > 1 else 0.0
            
            interp_factor = t 
            if interp_method == "ease-in": interp_factor = t * t
            elif interp_method == "ease-out": interp_factor = 1 - (1 - t) * (1 - t)
            elif interp_method == "ease-in-out":
                if t < 0.5: interp_factor = 2 * t * t 
                else: interp_factor = 1 - pow(-2 * t + 2, 2) / 2
            
            for i in range(len(tri_tuples_1)):
                x1, y1, c1 = tri_tuples_1[i]
                x2, y2, c2 = tri_tuples_2[i]
                new_x, new_y, new_c = 0.0, 0.0, 0.0

                if c1 > 0.05 and c2 > 0.05:
                    new_x = x1 + (x2 - x1) * interp_factor
                    new_y = y1 + (y2 - y1) * interp_factor
                    new_c = c1 + (c2 - c1) * interp_factor
                elif c1 > 0.05 and c2 <= 0.05: 
                    new_x, new_y = x1, y1
                    new_c = c1 * (1.0 - interp_factor) 
                elif c1 <= 0.05 and c2 > 0.05: 
                    new_x, new_y = x2, y2
                    new_c = c2 * interp_factor 
                interpolated_kps_for_frame.extend([new_x, new_y, new_c])
            keypoints_sequence.append(interpolated_kps_for_frame)
        return keypoints_sequence

    pose_1 = keypoints_1_dict.get('pose_keypoints_2d', [])
    face_1 = keypoints_1_dict.get('face_keypoints_2d', [])
    hand_left_1 = keypoints_1_dict.get('hand_left_keypoints_2d', [])
    hand_right_1 = keypoints_1_dict.get('hand_right_keypoints_2d', [])
    
    pose_2 = keypoints_2_dict.get('pose_keypoints_2d', [])
    face_2 = keypoints_2_dict.get('face_keypoints_2d', [])
    hand_left_2 = keypoints_2_dict.get('hand_left_keypoints_2d', [])
    hand_right_2 = keypoints_2_dict.get('hand_right_keypoints_2d', [])
    
    pose_sequence = interpolate_keypoint_set(pose_1, pose_2, frames, interpolation)
    face_sequence = interpolate_keypoint_set(face_1, face_2, frames, interpolation)
    hand_left_sequence = interpolate_keypoint_set(hand_left_1, hand_left_2, frames, interpolation)
    hand_right_sequence = interpolate_keypoint_set(hand_right_1, hand_right_2, frames, interpolation)

    combined_sequence = []
    for i in range(frames):
        combined_frame_data = {
            'pose_keypoints_2d': pose_sequence[i] if i < len(pose_sequence) else [],
            'face_keypoints_2d': face_sequence[i] if i < len(face_sequence) else [],
            'hand_left_keypoints_2d': hand_left_sequence[i] if i < len(hand_left_sequence) else [],
            'hand_right_keypoints_2d': hand_right_sequence[i] if i < len(hand_right_sequence) else []
        }
        combined_sequence.append(combined_frame_data)
    return combined_sequence

def extract_pose_keypoints(image_path: str | Path, include_face=True, include_hands=True, resolution: tuple[int,int]=(640,480)) -> dict:
    # import mediapipe as mp # Already imported at top
    # import cv2 # Already imported at top
    image = cv2.imread(str(image_path))
    if image is None:
        raise ValueError(f"Failed to load image: {image_path}")
    
    # Resize image before processing if its resolution differs significantly,
    # or ensure MediaPipe processes at a consistent internal resolution.
    # For now, assume MediaPipe handles various input sizes well, and keypoints are normalized.
    # The passed 'resolution' param will be used to scale normalized keypoints back to absolute.
    
    height, width = resolution[1], resolution[0] # For scaling output coords

    mp_holistic = mp.solutions.holistic
    # It's good practice to use a try-finally for resources like MediaPipe Holistic
    holistic_instance = mp_holistic.Holistic(static_image_mode=True, 
                                           min_detection_confidence=0.5, 
                                           min_tracking_confidence=0.5)
    try:
        # Convert BGR image to RGB for MediaPipe
        results = holistic_instance.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    finally:
        holistic_instance.close()
        
    keypoints = {}
    pose_kps = []
    if results.pose_landmarks:
        for lm in results.pose_landmarks.landmark:
            # lm.x, lm.y are normalized coordinates; scale them to target resolution
            pose_kps.extend([lm.x * width, lm.y * height, lm.visibility])
    keypoints['pose_keypoints_2d'] = pose_kps
    
    face_kps = []
    if include_face and results.face_landmarks:
        for lm in results.face_landmarks.landmark:
            face_kps.extend([lm.x * width, lm.y * height, lm.visibility if hasattr(lm, 'visibility') else 1.0]) # Some face landmarks might not have visibility
    keypoints['face_keypoints_2d'] = face_kps
    
    left_hand_kps = []
    if include_hands and results.left_hand_landmarks:
        for lm in results.left_hand_landmarks.landmark:
            left_hand_kps.extend([lm.x * width, lm.y * height, lm.visibility])
    keypoints['hand_left_keypoints_2d'] = left_hand_kps
    
    right_hand_kps = []
    if include_hands and results.right_hand_landmarks:
        for lm in results.right_hand_landmarks.landmark:
            right_hand_kps.extend([lm.x * width, lm.y * height, lm.visibility])
    keypoints['hand_right_keypoints_2d'] = right_hand_kps
    
    return keypoints

def create_pose_interpolated_guide_video(output_video_path: str | Path, resolution: tuple[int, int], total_frames: int,
                                           start_image_path: str | Path, end_image_path: str | Path,
                                           interpolation="linear", confidence_threshold=0.1,
                                           include_face=True, include_hands=True, fps=25):
    dprint(f"Creating pose interpolated guide: {output_video_path} from '{Path(start_image_path).name}' to '{Path(end_image_path).name}' ({total_frames} frames). First frame will be actual start image.")

    if total_frames <= 0:
        dprint(f"Video creation skipped for {output_video_path} as total_frames is {total_frames}.")
        return

    frames_list = []
    canvas_width, canvas_height = resolution

    first_visual_frame_np = image_to_frame(start_image_path, resolution)
    if first_visual_frame_np is None:
        print(f"Error loading start image {start_image_path} for guide video frame 0. Using black frame.")
        traceback.print_exc()
        first_visual_frame_np = create_color_frame(resolution, (0,0,0))
    frames_list.append(first_visual_frame_np)

    if total_frames > 1:
        try:
            # Pass the target resolution for keypoint scaling
            keypoints_from = extract_pose_keypoints(start_image_path, include_face, include_hands, resolution)
            keypoints_to = extract_pose_keypoints(end_image_path, include_face, include_hands, resolution)
        except Exception as e_extract:
            print(f"Error extracting keypoints for pose interpolation: {e_extract}. Filling remaining guide frames with black.")
            traceback.print_exc()
            black_frame = create_color_frame(resolution, (0,0,0))
            for _ in range(total_frames - 1):
                frames_list.append(black_frame)
            create_video_from_frames_list(frames_list, output_video_path, fps, resolution)
            return

        interpolated_sequence = transform_all_keypoints(keypoints_from, keypoints_to, total_frames, interpolation)
        
        # landmarkType for gen_skeleton_with_face_hands should indicate absolute coordinates
        # as extract_pose_keypoints now returns absolute coordinates scaled to 'resolution'
        landmark_type_for_gen = "AbsoluteCoords" 

        for i in range(1, total_frames):
            if i < len(interpolated_sequence):
                frame_data = interpolated_sequence[i]
                pose_kps = frame_data.get('pose_keypoints_2d', [])
                face_kps = frame_data.get('face_keypoints_2d', []) if include_face else []
                hand_left_kps = frame_data.get('hand_left_keypoints_2d', []) if include_hands else []
                hand_right_kps = frame_data.get('hand_right_keypoints_2d', []) if include_hands else []

                img = gen_skeleton_with_face_hands(
                    pose_kps, face_kps, hand_left_kps, hand_right_kps,
                    canvas_width, canvas_height, 
                    landmark_type_for_gen, # Keypoints are already absolute
                    confidence_threshold
                )
                frames_list.append(img)
            else:
                dprint(f"Warning: Interpolated sequence too short at index {i} for {output_video_path}. Appending black frame.")
                frames_list.append(create_color_frame(resolution, (0,0,0)))
    
    if len(frames_list) != total_frames:
        dprint(f"Warning: Generated {len(frames_list)} frames for {output_video_path}, expected {total_frames}. Adjusting.")
        if len(frames_list) < total_frames:
            last_frame = frames_list[-1] if frames_list else create_color_frame(resolution, (0,0,0))
            frames_list.extend([last_frame.copy() for _ in range(total_frames - len(frames_list))])
        else:
            frames_list = frames_list[:total_frames]

    if not frames_list:
        dprint(f"Error: No frames for video {output_video_path}. Skipping creation.")
        return

    create_video_from_frames_list(frames_list, output_video_path, fps, resolution)

# --- Debug Summary Video Helpers ---
def get_resized_frame(video_path_str: str, target_size: tuple[int, int], frame_ratio: float = 0.5) -> np.ndarray | None:
    """Extracts a frame (by ratio, e.g., 0.5 for middle) from a video and resizes it."""
    video_path = Path(video_path_str)
    if not video_path.exists() or video_path.stat().st_size == 0:
        dprint(f"GET_RESIZED_FRAME: Video not found or empty: {video_path_str}")
        placeholder = create_color_frame(target_size, (10, 10, 10)) # Dark grey
        cv2.putText(placeholder, "Not Found", (10, target_size[1] // 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)
        return placeholder

    cap = None
    try:
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            dprint(f"GET_RESIZED_FRAME: Could not open video: {video_path_str}")
            return create_color_frame(target_size, (20,20,20))

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames == 0:
            dprint(f"GET_RESIZED_FRAME: Video has 0 frames: {video_path_str}")
            return create_color_frame(target_size, (30,30,30))
        
        frame_to_get = int(total_frames * frame_ratio)
        frame_to_get = max(0, min(frame_to_get, total_frames - 1)) # Clamp
        
        cap.set(cv2.CAP_PROP_POS_FRAMES, float(frame_to_get))
        ret, frame = cap.read()
        if not ret or frame is None:
            dprint(f"GET_RESIZED_FRAME: Could not read frame {frame_to_get} from: {video_path_str}")
            return create_color_frame(target_size, (40,40,40))
        
        return cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)
    except Exception as e:
        dprint(f"GET_RESIZED_FRAME: Exception processing {video_path_str}: {e}")
        return create_color_frame(target_size, (50,50,50)) # Error color
    finally:
        if cap: cap.release()

def draw_multiline_text(image, text_lines, start_pos, font, font_scale, color, thickness, line_spacing):
    x, y = start_pos
    for i, line in enumerate(text_lines):
        line_y = y + (i * (cv2.getTextSize(line, font, font_scale, thickness)[0][1] + line_spacing))
        cv2.putText(image, line, (x, line_y), font, font_scale, color, thickness, cv2.LINE_AA)
    return image 

def generate_debug_summary_video(segments_data: list[dict], output_path: str | Path, fps: int, 
                                 num_frames_for_collage: int, 
                                 target_thumb_size: tuple[int, int] = (320, 180)):
    if not DEBUG_MODE: return # Only run if debug mode is on
    if not segments_data:
        dprint("GENERATE_DEBUG_SUMMARY_VIDEO: No segment data provided.")
        return

    dprint(f"Generating animated debug collage with {num_frames_for_collage} frames, at {fps} FPS.")

    thumb_w, thumb_h = target_thumb_size
    padding = 10
    header_h = 50 
    text_line_h_approx = 20
    max_settings_lines = 6
    settings_area_h = (text_line_h_approx * max_settings_lines) + padding 

    num_segments = len(segments_data)
    col_w = thumb_w + (2 * padding)
    canvas_w = num_segments * col_w
    canvas_h = header_h + (thumb_h * 2) + (padding * 3) + settings_area_h + padding
    
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale_small = 0.4
    font_scale_title = 0.6
    text_color = (230, 230, 230)
    title_color = (255, 255, 255)
    line_thickness = 1

    overall_static_template_canvas = np.full((canvas_h, canvas_w, 3), (30, 30, 30), dtype=np.uint8) 
    for idx, seg_data in enumerate(segments_data):
        col_x_start = idx * col_w
        center_x_col = col_x_start + col_w // 2
        title_text = f"Segment {seg_data['segment_index']}"
        (tw, th), _ = cv2.getTextSize(title_text, font, font_scale_title, line_thickness)
        cv2.putText(overall_static_template_canvas, title_text, (center_x_col - tw//2, header_h - padding), font, font_scale_title, title_color, line_thickness, cv2.LINE_AA)
        
        y_offset = header_h
        cv2.putText(overall_static_template_canvas, "Input Guide", (col_x_start + padding, y_offset + text_line_h_approx), font, font_scale_small, text_color, line_thickness)
        y_offset += thumb_h + padding
        cv2.putText(overall_static_template_canvas, "Headless Output", (col_x_start + padding, y_offset + text_line_h_approx), font, font_scale_small, text_color, line_thickness)
        y_offset += thumb_h + padding

        settings_y_start = y_offset
        cv2.putText(overall_static_template_canvas, "Settings:", (col_x_start + padding, settings_y_start + text_line_h_approx), font, font_scale_small, text_color, line_thickness)
        settings_text_lines = []
        payload = seg_data.get("task_payload", {})
        settings_text_lines.append(f"Task ID: {payload.get('task_id', 'N/A')[:10]}...")
        prompt_short = payload.get('prompt', 'N/A')[:35] + ("..." if len(payload.get('prompt', '')) > 35 else "")
        settings_text_lines.append(f"Prompt: {prompt_short}")
        settings_text_lines.append(f"Seed: {payload.get('seed', 'N/A')}, Frames: {payload.get('frames', 'N/A')}")
        settings_text_lines.append(f"Resolution: {payload.get('resolution', 'N/A')}")
        draw_multiline_text(overall_static_template_canvas, settings_text_lines[:max_settings_lines], 
                            (col_x_start + padding, settings_y_start + text_line_h_approx + padding), 
                            font, font_scale_small, text_color, line_thickness, 5)

    error_placeholder_frame = create_color_frame(target_thumb_size, (50, 0, 0)) 
    cv2.putText(error_placeholder_frame, "ERR", (10, target_thumb_size[1]//2), font, 0.8, (255,255,255), 1)
    not_found_placeholder_frame = create_color_frame(target_thumb_size, (0, 50, 0)) 
    cv2.putText(not_found_placeholder_frame, "N/A", (10, target_thumb_size[1]//2), font, 0.8, (255,255,255), 1)
    static_thumbs_cache = {}
    for seg_idx_cache, seg_data_cache in enumerate(segments_data):
        guide_thumb = get_resized_frame(seg_data_cache["guide_video_path"], target_thumb_size, frame_ratio=0.5)
        output_thumb = get_resized_frame(seg_data_cache["raw_headless_output_path"], target_thumb_size, frame_ratio=0.5)
        
        static_thumbs_cache[seg_idx_cache] = {
            'guide': guide_thumb if guide_thumb is not None else not_found_placeholder_frame,
            'output': output_thumb if output_thumb is not None else not_found_placeholder_frame
        }

    writer = None
    try:
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(str(output_path), fourcc, float(fps), (canvas_w, canvas_h))
        if not writer.isOpened():
            dprint(f"GENERATE_DEBUG_SUMMARY_VIDEO: Failed to open VideoWriter for {output_path}")
            return
        
        dprint(f"GENERATE_DEBUG_SUMMARY_VIDEO: Writing sequentially animated collage to {output_path}")

        for active_seg_idx in range(num_segments):
            dprint(f"Animating segment {active_seg_idx} in collage...")
            caps_for_active_segment = {'guide': None, 'output': None, 'last_frames': {}}
            video_paths_to_load = {
                'guide': segments_data[active_seg_idx]["guide_video_path"],
                'output': segments_data[active_seg_idx]["raw_headless_output_path"]
            }
            for key, path_str in video_paths_to_load.items():
                p = Path(path_str)
                if p.exists() and p.stat().st_size > 0:
                    cap_video = cv2.VideoCapture(str(p))
                    if cap_video.isOpened():
                        caps_for_active_segment[key] = cap_video
                        ret, frame = cap_video.read(); 
                        caps_for_active_segment['last_frames'][key] = cv2.resize(frame, target_thumb_size, cv2.INTER_AREA) if ret and frame is not None else error_placeholder_frame
                        cap_video.set(cv2.CAP_PROP_POS_FRAMES, 0.0)
                    else: caps_for_active_segment['last_frames'][key] = error_placeholder_frame
                else: caps_for_active_segment['last_frames'][key] = not_found_placeholder_frame

            for frame_num in range(num_frames_for_collage):
                current_frame_canvas = overall_static_template_canvas.copy()

                for display_seg_idx in range(num_segments):
                    col_x_start = display_seg_idx * col_w
                    current_y_pos = header_h
                    
                    videos_to_composite = [None, None] # guide, output

                    if display_seg_idx == active_seg_idx:
                        if caps_for_active_segment['guide']:
                            ret, frame = caps_for_active_segment['guide'].read()
                            if ret and frame is not None: videos_to_composite[0] = cv2.resize(frame, target_thumb_size); caps_for_active_segment['last_frames']['guide'] = videos_to_composite[0]
                            else: videos_to_composite[0] = caps_for_active_segment['last_frames'].get('guide', error_placeholder_frame)
                        else: videos_to_composite[0] = caps_for_active_segment['last_frames'].get('guide', not_found_placeholder_frame)
                        if caps_for_active_segment['output']:
                            ret, frame = caps_for_active_segment['output'].read()
                            if ret and frame is not None: videos_to_composite[1] = cv2.resize(frame, target_thumb_size); caps_for_active_segment['last_frames']['output'] = videos_to_composite[1]
                            else: videos_to_composite[1] = caps_for_active_segment['last_frames'].get('output', error_placeholder_frame)
                        else: videos_to_composite[1] = caps_for_active_segment['last_frames'].get('output', not_found_placeholder_frame)
                    else:
                        videos_to_composite[0] = static_thumbs_cache[display_seg_idx]['guide']
                        videos_to_composite[1] = static_thumbs_cache[display_seg_idx]['output']

                    current_frame_canvas[current_y_pos : current_y_pos + thumb_h, col_x_start + padding : col_x_start + padding + thumb_w] = videos_to_composite[0]
                    current_y_pos += thumb_h + padding
                    current_frame_canvas[current_y_pos : current_y_pos + thumb_h, col_x_start + padding : col_x_start + padding + thumb_w] = videos_to_composite[1]
                
                writer.write(current_frame_canvas)
            
            if caps_for_active_segment['guide']: caps_for_active_segment['guide'].release()
            if caps_for_active_segment['output']: caps_for_active_segment['output'].release()
            dprint(f"Finished animating segment {active_seg_idx} in collage.")

        dprint(f"GENERATE_DEBUG_SUMMARY_VIDEO: Finished writing sequentially animated debug collage.")

    except Exception as e:
        dprint(f"GENERATE_DEBUG_SUMMARY_VIDEO: Exception during video writing: {e} - {traceback.format_exc()}")
    finally:
        if writer: writer.release()
        dprint("GENERATE_DEBUG_SUMMARY_VIDEO: Video writer released.")


def download_file(url, dest_folder, filename):
    dest_path = Path(dest_folder) / filename
    if dest_path.exists():
        # Validate existing file before assuming it's good
        if filename.endswith('.safetensors') or 'lora' in filename.lower():
            is_valid, validation_msg = validate_lora_file(dest_path, filename)
            if is_valid:
                print(f"[INFO] File {filename} already exists and is valid in {dest_folder}. {validation_msg}")
                return True
            else:
                print(f"[WARNING] Existing file {filename} failed validation ({validation_msg}). Re-downloading...")
                dest_path.unlink()
        else:
            print(f"[INFO] File {filename} already exists in {dest_folder}.")
            return True
    
    # Use huggingface_hub for HuggingFace URLs for better reliability
    if "huggingface.co" in url:
        try:
            from huggingface_hub import hf_hub_download
            from urllib.parse import urlparse
            
            # Parse HuggingFace URL to extract repo_id and filename
            # Format: https://huggingface.co/USER/REPO/resolve/BRANCH/FILENAME
            parsed = urlparse(url)
            path_parts = parsed.path.strip('/').split('/')
            
            if len(path_parts) >= 4 and path_parts[2] == 'resolve':
                repo_id = f"{path_parts[0]}/{path_parts[1]}"
                branch = path_parts[3] if len(path_parts) > 4 else "main"
                hf_filename = '/'.join(path_parts[4:]) if len(path_parts) > 4 else filename
                
                print(f"Downloading {filename} from HuggingFace repo {repo_id} using hf_hub_download...")
                
                # Download using huggingface_hub with automatic checksums and resumption
                downloaded_path = hf_hub_download(
                    repo_id=repo_id,
                    filename=hf_filename,
                    revision=branch,
                    cache_dir=str(dest_folder),
                    resume_download=True,
                    local_files_only=False
                )
                
                # Copy from HF cache to target location if different
                if Path(downloaded_path) != dest_path:
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    import shutil
                    shutil.copy2(downloaded_path, dest_path)
                
                # Validate the downloaded file
                if filename.endswith('.safetensors') or 'lora' in filename.lower():
                    is_valid, validation_msg = validate_lora_file(dest_path, filename)
                    if not is_valid:
                        print(f"[ERROR] Downloaded file {filename} failed validation: {validation_msg}")
                        dest_path.unlink(missing_ok=True)
                        return False
                    print(f"Successfully downloaded and validated {filename}. {validation_msg}")
                else:
                    print(f"Successfully downloaded {filename} with integrity verification.")
                return True
                
        except ImportError:
            print(f"[WARNING] huggingface_hub not available, falling back to requests for {url}")
        except Exception as e:
            print(f"[WARNING] HuggingFace download failed for {filename}: {e}, falling back to requests")
    
    # Fallback to requests with basic integrity checks
    try:
        print(f"Downloading {filename} from {url} to {dest_folder}...")
        response = requests.get(url, stream=True)
        response.raise_for_status() # Raise an exception for HTTP errors
        
        # Get expected content length for verification
        expected_size = int(response.headers.get('content-length', 0))
        
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        with open(dest_path, 'wb') as f:
            downloaded_size = 0
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
                downloaded_size += len(chunk)
        
        # Verify download integrity
        actual_size = dest_path.stat().st_size
        if expected_size > 0 and actual_size != expected_size:
            print(f"[ERROR] Size mismatch for {filename}: expected {expected_size}, got {actual_size}")
            dest_path.unlink(missing_ok=True)
            return False
        
        # Use comprehensive validation for LoRA files
        if filename.endswith('.safetensors') or 'lora' in filename.lower():
            is_valid, validation_msg = validate_lora_file(dest_path, filename)
            if not is_valid:
                print(f"[ERROR] Downloaded file {filename} failed validation: {validation_msg}")
                dest_path.unlink(missing_ok=True)
                return False
            print(f"Successfully downloaded and validated {filename}. {validation_msg}")
        else:
            # For non-LoRA safetensors files, do basic format check
            if filename.endswith('.safetensors'):
                try:
                    import safetensors.torch as st
                    with st.safe_open(dest_path, framework="pt") as f:
                        pass  # Just verify it can be opened
                    print(f"Successfully downloaded and verified safetensors file {filename}.")
                except ImportError:
                    print(f"[WARNING] safetensors not available for verification of {filename}")
                except Exception as e:
                    print(f"[ERROR] Downloaded safetensors file {filename} appears corrupted: {e}")
                    dest_path.unlink(missing_ok=True)
                    return False
            else:
                print(f"Successfully downloaded {filename}.")
        
        return True
        
    except Exception as e:
        print(f"[ERROR] Failed to download {filename}: {e}")
        if dest_path.exists(): # Attempt to clean up partial download
            try: os.remove(dest_path)
            except: pass
        return False

# Added to provide a unique target path generator for files.
def _get_unique_target_path(target_dir: Path, base_name: str, extension: str) -> Path:
    """Generates a unique target Path in the given directory by appending a timestamp and random string."""
    target_dir = Path(target_dir)
    target_dir.mkdir(parents=True, exist_ok=True)
    timestamp_short = datetime.now().strftime("%H%M%S")
    # Use a short UUID/random string to significantly reduce collision probability with just timestamp
    unique_suffix = uuid.uuid4().hex[:6]
    
    # Construct the filename
    # Ensure extension has a leading dot
    if extension and not extension.startswith('.'):
        extension = '.' + extension
    
    filename = f"{base_name}_{timestamp_short}_{unique_suffix}{extension}"
    return target_dir / filename

def _download_file_if_url(
    file_url_or_path: str,
    download_target_dir: Path | str | None,
    task_id_for_logging: str | None = "generic_task",
    descriptive_name: str | None = None,
    default_extension: str = ".jpg",
    default_stem: str = "file",
    file_type_label: str = "file",
    timeout: int = 300
) -> str:
    """
    Generic function to download a file from URL if needed.
    
    Args:
        file_url_or_path: URL or local path
        download_target_dir: Directory to save downloaded file
        task_id_for_logging: Task ID for logging
        descriptive_name: Optional descriptive name for the file
        default_extension: Default file extension if none detected
        default_stem: Default stem for filename if none detected
        file_type_label: Label for logging (e.g., "image", "video")
        timeout: Request timeout in seconds
        
    Returns:
        Local file path string if downloaded, otherwise returns the original string
    """
    if not file_url_or_path:
        return file_url_or_path

    parsed_url = urlparse(file_url_or_path)
    if parsed_url.scheme in ['http', 'https'] and download_target_dir:
        target_dir_path = Path(download_target_dir)
        try:
            target_dir_path.mkdir(parents=True, exist_ok=True)
            dprint(f"Task {task_id_for_logging}: Downloading {file_type_label} from URL: {file_url_or_path} to {target_dir_path.resolve()}")
            
            # Use a session for potential keep-alive and connection pooling
            with requests.Session() as s:
                response = s.get(file_url_or_path, stream=True, timeout=timeout)
                response.raise_for_status()

            original_filename = Path(parsed_url.path).name
            original_suffix = Path(original_filename).suffix if Path(original_filename).suffix else default_extension
            if not original_suffix.startswith('.'):
                original_suffix = '.' + original_suffix
            
            # Use descriptive naming if provided, otherwise fall back to improved default
            if descriptive_name:
                base_name_for_download = descriptive_name[:50]  # Limit length
            else:
                # Improved default naming
                cleaned_stem = Path(original_filename).stem[:30] if Path(original_filename).stem else default_stem
                base_name_for_download = f"input_{cleaned_stem}"
            
            # _get_unique_target_path expects a Path object for target_dir
            local_file_path = _get_unique_target_path(target_dir_path, base_name_for_download, original_suffix)
            
            with open(local_file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            dprint(f"Task {task_id_for_logging}: Successfully downloaded {file_type_label} to {local_file_path}")
            return str(local_file_path)
            
        except requests.exceptions.RequestException as e:
            dprint(f"Task {task_id_for_logging}: ERROR downloading {file_type_label} from {file_url_or_path}: {e}")
            return file_url_or_path  # Return original URL on failure
        except Exception as e_dl:
            dprint(f"Task {task_id_for_logging}: ERROR saving {file_type_label} to {target_dir_path}: {e_dl}")
            traceback.print_exc()
            return file_url_or_path  # Return original URL on failure
    else:
        # Not a URL, or no download directory provided
        return file_url_or_path


def download_video_if_url(video_url_or_path: str, download_target_dir: Path | str | None, task_id_for_logging: str | None = "generic_task", descriptive_name: str | None = None) -> str:
    """
    Checks if the given string is an HTTP/HTTPS URL. If so, and if download_target_dir is provided,
    downloads the video to a unique path within download_target_dir.
    Returns the local file path string if downloaded, otherwise returns the original string.
    """
    return _download_file_if_url(
        video_url_or_path,
        download_target_dir,
        task_id_for_logging,
        descriptive_name,
        default_extension=".mp4",
        default_stem="structure_video",
        file_type_label="video",
        timeout=600  # 10 min timeout for larger videos
    )


def download_image_if_url(image_url_or_path: str, download_target_dir: Path | str | None, task_id_for_logging: str | None = "generic_task", debug_mode: bool = False, descriptive_name: str | None = None) -> str:
    """
    Checks if the given string is an HTTP/HTTPS URL. If so, and if download_target_dir is provided,
    downloads the image to a unique path within download_target_dir.
    Returns the local file path string if downloaded, otherwise returns the original string.
    
    Note: debug_mode parameter is kept for backwards compatibility but not currently used.
    """
    # Use the generic download function
    return _download_file_if_url(
        image_url_or_path,
        download_target_dir,
        task_id_for_logging,
        descriptive_name,
        default_extension=".jpg",
        default_stem="image",
        file_type_label="image",
        timeout=300  # 5 min timeout for images
    )

def image_to_frame(image_path_str: str | Path, target_resolution_wh: tuple[int, int] | None = None, task_id_for_logging: str | None = "generic_task", image_download_dir: Path | str | None = None, debug_mode: bool = False) -> np.ndarray | None:
    """
    Load an image, optionally resize, and convert to BGR NumPy array.
    If image_path_str is a URL and image_download_dir is provided, it attempts to download it first.
    """
    resolved_image_path_str = image_path_str # Default to original path

    if isinstance(image_path_str, str): # Only attempt download if it's a string (potentially a URL)
        resolved_image_path_str = download_image_if_url(image_path_str, image_download_dir, task_id_for_logging, debug_mode)
    
    image_path = Path(resolved_image_path_str)

    if not image_path.exists():
        dprint(f"Task {task_id_for_logging}: Image file not found at {image_path} (original input: {image_path_str}).")
        return None
    try:
        img = Image.open(image_path).convert("RGB") # Ensure RGB for consistent processing
        if target_resolution_wh:
            img = img.resize(target_resolution_wh, Image.Resampling.LANCZOS)
        return cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
    except Exception as e:
        dprint(f"Error loading image {image_path} (original input {image_path_str}): {e}")
        return None

def apply_strength_to_image(
    image_path_input: Path | str, # Changed name to avoid confusion
    strength: float,
    output_path: Path,
    target_resolution_wh: tuple[int, int] | None,
    task_id_for_logging: str | None = "generic_task",
    image_download_dir: Path | str | None = None
) -> Path | None:
    """
    Applies a brightness adjustment (strength) to an image, optionally resizes, and saves it.
    If image_path_input is a URL string and image_download_dir is provided, it attempts to download it first.
    """
    resolved_image_path_str = str(image_path_input) 

    if isinstance(image_path_input, str):
         resolved_image_path_str = download_image_if_url(image_path_input, image_download_dir, task_id_for_logging)
    # Check if image_path_input was a Path object representing a URL (less common for this function)
    elif isinstance(image_path_input, Path) and image_path_input.as_posix().startswith(('http://', 'https://')):
         resolved_image_path_str = download_image_if_url(image_path_input.as_posix(), image_download_dir, task_id_for_logging)

    actual_image_path = Path(resolved_image_path_str)

    if not actual_image_path.exists():
        dprint(f"Task {task_id_for_logging}: Source image not found at {actual_image_path} (original input: {image_path_input}) for strength application.")
        return None
    try:
        # Open the potentially downloaded or original local image
        img = Image.open(actual_image_path).convert("RGB") # Ensure RGB
        
        if target_resolution_wh:
            img = img.resize(target_resolution_wh, Image.Resampling.LANCZOS)
        
        # Apply the strength factor using PIL.ImageEnhance for brightness
        enhancer = ImageEnhance.Brightness(img)
        processed_img = enhancer.enhance(strength) # 'strength' is the factor for brightness
        
        # Save the adjusted image
        output_path.parent.mkdir(parents=True, exist_ok=True)
        processed_img.save(output_path) # Save PIL image directly

        dprint(f"Task {task_id_for_logging}: Applied strength {strength} to {actual_image_path.name}, saved to {output_path.name} with resolution {target_resolution_wh if target_resolution_wh else 'original'}")
        return output_path
    except Exception as e:
        dprint(f"Task {task_id_for_logging}: Error in _apply_strength_to_image for {actual_image_path}: {e}")
        traceback.print_exc()
        return None

def _copy_to_folder_with_unique_name(source_path: Path, target_dir: Path, base_name: str, extension: str) -> Path | None:
    """Copies a file to a target directory with a unique name based on timestamp and random string."""
    if not source_path:
        dprint(f"COPY: Source path is None for {base_name}{extension}. Skipping copy.")
        return None
    
    source_path_obj = Path(source_path)
    if not source_path_obj.exists():
        dprint(f"COPY: Source file {source_path_obj} does not exist. Skipping copy.")
        return None

    # Sanitize extension for _get_unique_target_path
    actual_extension = source_path_obj.suffix if source_path_obj.suffix else extension
    if not actual_extension.startswith('.'):
        actual_extension = '.' + actual_extension

    # Determine unique target path using the new helper
    target_file = _get_unique_target_path(target_dir, base_name, actual_extension)
    
    try:
        # target_dir.mkdir(parents=True, exist_ok=True) # _get_unique_target_path handles this
        shutil.copy2(str(source_path_obj), str(target_file))
        dprint(f"COPY: Copied {source_path_obj.name} to {target_file}")
        return target_file # Return the path of the copied file
    except Exception as e_copy:
        dprint(f"COPY: Failed to copy {source_path_obj} to {target_file}: {e_copy}")
        return None
    

def get_image_dimensions_pil(image_path: str | Path) -> tuple[int, int]:
    """Returns the dimensions of an image file as (width, height)."""
    with Image.open(image_path) as img:
        return img.size

# Added to adjust the brightness of an image/frame.
def adjust_frame_brightness(frame: np.ndarray, factor: float) -> np.ndarray:
    """Adjusts the brightness of a given frame.
    The 'factor' is interpreted as a delta from the CLI argument:
    - Positive factor (e.g., 0.1) makes it darker (target_alpha = 1.0 - 0.1 = 0.9).
    - Negative factor (e.g., -0.1) makes it brighter (target_alpha = 1.0 - (-0.1) = 1.1).
    - Zero factor means no change (target_alpha = 1.0).
    """
    # Convert the CLI-style factor to an alpha for cv2.convertScaleAbs
    # CLI factor: positive = darker, negative = brighter
    # cv2 alpha: >1 = brighter, <1 = darker
    cv2_alpha = 1.0 - factor 
    return cv2.convertScaleAbs(frame, alpha=cv2_alpha, beta=0)

def get_unique_target_path(target_dir: Path, name_stem: str, suffix: str) -> Path:
    """Generates a unique target Path in the given directory by appending a number if needed."""
    if not suffix.startswith('.'):
        suffix = f".{suffix}"
    
    final_path = target_dir / f"{name_stem}{suffix}"
    counter = 1
    while final_path.exists():
        final_path = target_dir / f"{name_stem}_{counter}{suffix}"
        counter += 1
    return final_path

def load_pil_images(
    paths_list_or_str: list[str] | str,
    wgp_convert_func: callable,
    image_download_dir: Path | str | None,
    task_id_for_log: str,
    dprint: callable
) -> list[Any] | None:
    """
    Loads one or more images from paths or URLs, downloads them if necessary,
    and applies a conversion function.
    """
    if not paths_list_or_str:
        return None
    
    paths_list = paths_list_or_str if isinstance(paths_list_or_str, list) else [paths_list_or_str]
    images = []

    for p_str in paths_list:
        local_p_str = download_image_if_url(p_str, image_download_dir, task_id_for_log)
        if not local_p_str:
            dprint(f"[Task {task_id_for_log}] Skipping image as download_image_if_url returned nothing for: {p_str}")
            continue

        p = Path(local_p_str.strip())
        if not p.is_file():
            dprint(f"[Task {task_id_for_log}] load_pil_images: Image file not found after potential download: {p} (original: {p_str})")
            continue
        try:
            img = Image.open(p)
            images.append(wgp_convert_func(img))
        except Exception as e:
            print(f"[WARNING] Failed to load image {p}: {e}")
            
    return images if images else None

def _normalize_activated_loras_list(current_activated) -> list:
    """Helper to ensure activated_loras is a proper list."""
    if not isinstance(current_activated, list):
        try:
            return [str(item).strip() for item in str(current_activated).split(',') if item.strip()]
        except Exception:
            return []
    return current_activated

def _apply_special_lora_settings(task_id: str, lora_type: str, lora_basename: str, default_steps: int, 
                                 guidance_scale: float, flow_shift: float, ui_defaults: dict, 
                                 task_params_dict: dict, tea_cache_setting: float = None):
    """
    Shared helper to apply special LoRA settings (CausVid, LightI2X, etc.) to ui_defaults.
    """
    print(f"[Task ID: {task_id}] Applying {lora_type} LoRA settings.")
    
    # [STEPS DEBUG] Add detailed debug for steps logic
    dprint(f"[STEPS DEBUG] {lora_type}: task_params_dict keys: {list(task_params_dict.keys())}")
    if "steps" in task_params_dict:
        dprint(f"[STEPS DEBUG] {lora_type}: Found 'steps' = {task_params_dict['steps']}")
    if "num_inference_steps" in task_params_dict:
        dprint(f"[STEPS DEBUG] {lora_type}: Found 'num_inference_steps' = {task_params_dict['num_inference_steps']}")
    if "video_length" in task_params_dict:
        dprint(f"[STEPS DEBUG] {lora_type}: Found 'video_length' = {task_params_dict['video_length']}")
    
    # Handle steps logic
    if "steps" in task_params_dict:
        ui_defaults["num_inference_steps"] = task_params_dict["steps"]
        print(f"[Task ID: {task_id}] {lora_type} task using specified steps: {ui_defaults['num_inference_steps']}")
    elif "num_inference_steps" in task_params_dict:
        ui_defaults["num_inference_steps"] = task_params_dict["num_inference_steps"]
        print(f"[Task ID: {task_id}] {lora_type} task using specified num_inference_steps: {ui_defaults['num_inference_steps']}")
    else:
        ui_defaults["num_inference_steps"] = default_steps
        print(f"[Task ID: {task_id}] {lora_type} task defaulting to steps: {ui_defaults['num_inference_steps']}")
    
    # Set guidance and flow shift
    ui_defaults["guidance_scale"] = guidance_scale
    ui_defaults["flow_shift"] = flow_shift
    
    # Set tea cache if specified
    if tea_cache_setting is not None:
        ui_defaults["tea_cache_setting"] = tea_cache_setting
    
    # Handle LoRA activation
    current_activated = _normalize_activated_loras_list(ui_defaults.get("activated_loras", []))
    
    if lora_basename not in current_activated:
        current_activated.append(lora_basename)
    ui_defaults["activated_loras"] = current_activated
    
    # Handle multipliers - simple approach for build_task_state
    current_multipliers_str = ui_defaults.get("loras_multipliers", "")
    multipliers_list = [m.strip() for m in current_multipliers_str.split(" ") if m.strip()] if current_multipliers_str else []
    while len(multipliers_list) < len(current_activated):
        multipliers_list.insert(0, "1.0")
    ui_defaults["loras_multipliers"] = " ".join(multipliers_list)

# --- SM_RESTRUCTURE: Function moved from worker.py ---
def _get_task_type_directory(task_type: str) -> str:
    """
    Map task types to their output subdirectories.

    Single-level directory structure: outputs/{task_type}/{files}
    Examples:
    - vace ‚Üí outputs/vace/
    - travel_orchestrator ‚Üí outputs/travel_orchestrator/
    - extract_frame ‚Üí outputs/extract_frame/

    Args:
        task_type: The task type string (e.g., 'vace', 't2v', 'travel_orchestrator')

    Returns:
        Task type as directory name (e.g., 'vace', 'travel_orchestrator')
    """
    # Simply return the task type as the directory name
    # This creates outputs/{task_type}/ structure
    return task_type if task_type else 'misc'


def prepare_output_path(
    task_id: str,
    filename: str,
    main_output_dir_base: Path,
    task_type: str | None = None,  # NEW PARAMETER for Phase 2
    *,
    dprint=lambda *_: None,
    custom_output_dir: str | Path | None = None
) -> tuple[Path, str]:
    """
    Prepares the output path for a task's artifact.

    If `custom_output_dir` is provided, it's used as the base. Otherwise,
    the output is placed in a subdirectory based on task_type (Phase 2)
    or directly in `main_output_dir_base` (Phase 1 backwards compatibility).

    Args:
        task_id: Unique task identifier
        filename: Output filename
        main_output_dir_base: Base output directory (from worker --main-output-dir)
        task_type: Optional task type for subdirectory organization (Phase 2)
        dprint: Debug print function
        custom_output_dir: Optional custom output directory (overrides all)

    Returns:
        Tuple of (Path object for file location, string path for database)
    """
    # Import DB configuration lazily to avoid circular dependencies.
    try:
        from source import db_operations as db_ops  # type: ignore
    except Exception:  # pragma: no cover
        db_ops = None

    # Decide base directory for the file
    if custom_output_dir:
        output_dir_for_task = Path(custom_output_dir)
        dprint(f"Task {task_id}: Using custom output directory: {output_dir_for_task}")
    else:
        # Phase 2: Create task-type-specific subdirectory structure
        if task_type:
            type_subdir = _get_task_type_directory(task_type)
            output_dir_for_task = main_output_dir_base / type_subdir
            dprint(
                f"Task {task_id}: Using task-type subdirectory: {output_dir_for_task} "
                f"(task_type='{task_type}' ‚Üí '{type_subdir}')"
            )
        else:
            # Backwards compatibility: No task_type provided, use root directory
            output_dir_for_task = main_output_dir_base
            dprint(
                f"Task {task_id}: No task_type provided, using root output directory: {output_dir_for_task} "
                f"(Phase 1 backwards compatibility)"
            )

        # To avoid name collisions we prefix the filename with the task_id
        # Skip prefixing for files with UUID patterns (they guarantee uniqueness)
        import re
        # Match UUID pattern: _HHMMSS_uuid6.ext
        uuid_pattern = r'_\d{6}_[a-f0-9]{6}\.(mp4|png|jpg|jpeg)$'
        has_uuid_pattern = re.search(uuid_pattern, filename, re.IGNORECASE)

        if not filename.startswith(task_id) and not has_uuid_pattern:
            filename = f"{task_id}_{filename}"

    output_dir_for_task.mkdir(parents=True, exist_ok=True)

    final_save_path = output_dir_for_task / filename

    # Handle filename conflicts by adding _1, _2, etc.
    if final_save_path.exists():
        stem = final_save_path.stem
        suffix = final_save_path.suffix
        counter = 1
        while final_save_path.exists():
            new_filename = f"{stem}_{counter}{suffix}"
            final_save_path = output_dir_for_task / new_filename
            counter += 1
        dprint(f"Task {task_id}: Filename conflict resolved - using {final_save_path.name}")

    # Build DB path string - use relative path to current working directory
    try:
        db_output_location = str(final_save_path.relative_to(Path.cwd()))
    except ValueError:
        db_output_location = str(final_save_path.resolve())

    dprint(f"Task {task_id}: final_save_path='{final_save_path}', db_output_location='{db_output_location}'")

    return final_save_path, db_output_location

def sanitize_filename_for_storage(filename: str) -> str:
    """
    Sanitizes a filename to be safe for storage systems like Supabase Storage.
    
    Removes characters that are invalid for S3/Supabase storage keys:
    - Control characters (0x00-0x1F, 0x7F-0x9F)
    - Special characters: ¬ß ¬Æ ¬© ‚Ñ¢ @ ¬∑ ¬∫ ¬Ω ¬æ ¬ø ¬° ~ and others
    - Path separators and other problematic characters
    
    Args:
        filename: Original filename that may contain unsafe characters
        
    Returns:
        Sanitized filename safe for storage systems
    """
    import re
    
    # Define characters to remove (includes the ¬ß character causing the issue)
    # This is based on S3/Supabase storage key restrictions and common filesystem issues
    unsafe_chars = r'[¬ß¬Æ¬©‚Ñ¢@¬∑¬∫¬Ω¬æ¬ø¬°~\x00-\x1F\x7F-\x9F<>:"/\\|?*,]'
    
    # Replace unsafe characters with empty string
    sanitized = re.sub(unsafe_chars, '', filename)
    
    # Replace multiple consecutive spaces/dots with single ones
    sanitized = re.sub(r'[ \.]{2,}', ' ', sanitized)
    
    # Remove leading/trailing whitespace and dots
    sanitized = sanitized.strip(' .')
    
    # Ensure we have a non-empty filename
    if not sanitized:
        sanitized = "sanitized_file"
    
    return sanitized

def prepare_output_path_with_upload(
    task_id: str,
    filename: str,
    main_output_dir_base: Path,
    task_type: str | None = None,  # NEW PARAMETER for Phase 2
    *,
    dprint=lambda *_: None,
    custom_output_dir: str | Path | None = None
) -> tuple[Path, str]:
    """
    Prepares the output path for a task's artifact and handles Supabase upload if configured.

    Args:
        task_id: Unique task identifier
        filename: Output filename
        main_output_dir_base: Base output directory (from worker --main-output-dir)
        task_type: Optional task type for subdirectory organization (Phase 2)
        dprint: Debug print function
        custom_output_dir: Optional custom output directory (overrides all)

    Returns:
        tuple[Path, str]: (local_file_path, db_output_location)
        - local_file_path: Where to save the file locally (for generation)
        - db_output_location: What to store in the database (local path or Supabase URL)
    """
    # Import DB configuration lazily to avoid circular dependencies.
    try:
        from source import db_operations as db_ops  # type: ignore
    except Exception:  # pragma: no cover
        db_ops = None

    # Sanitize filename BEFORE any processing to prevent storage upload issues
    original_filename = filename
    sanitized_filename = sanitize_filename_for_storage(filename)

    if original_filename != sanitized_filename:
        dprint(f"Task {task_id}: Sanitized filename '{original_filename}' -> '{sanitized_filename}'")

    # First, get the local path where we'll save the file (using sanitized filename)
    # PHASE 2: Forward task_type parameter
    local_save_path, initial_db_location = prepare_output_path(
        task_id, sanitized_filename, main_output_dir_base,
        task_type=task_type,  # Forward task_type for Phase 2
        dprint=dprint, custom_output_dir=custom_output_dir
    )

    # Return the local path for now - we'll handle Supabase upload after file is created
    return local_save_path, initial_db_location

def upload_and_get_final_output_location(
    local_file_path: Path,
    supabase_object_name: str,  # This parameter is now unused but kept for compatibility
    initial_db_location: str,
    *,
    dprint=lambda *_: None
) -> str:
    """
    Returns the local file path. Upload is now handled by the edge function.
    
    Args:
        local_file_path: Path to the local file
        supabase_object_name: Unused (kept for compatibility)
        initial_db_location: The initial DB location (local path)
        dprint: Debug print function
        
    Returns:
        str: Local file path (upload now handled by edge function)
    """
    # Edge function will handle the upload, so we just return the local path
    dprint(f"File ready for edge function upload: {local_file_path}")
    return str(local_file_path.resolve())


def upload_intermediate_file_to_storage(
    local_file_path: Path,
    task_id: str,
    filename: str,
    *,
    dprint=lambda *_: None
) -> str | None:
    """
    Upload an intermediate file to Supabase storage for cross-worker access.
    
    This is used when orchestrators create intermediate files (like reversed videos)
    that need to be accessible by child tasks running on different workers.
    
    Includes retry logic for transient failures (502/503/504, timeouts, network errors).
    
    Args:
        local_file_path: Path to the local file to upload
        task_id: Task ID for organizing uploads
        filename: Filename to use in storage
        dprint: Debug print function
        
    Returns:
        Public URL of the uploaded file, or None on failure
    """
    import httpx
    import mimetypes
    
    # Retry configuration
    RETRYABLE_STATUS_CODES = {502, 503, 504}
    MAX_RETRIES = 3
    
    # Check if Supabase is configured
    SUPABASE_URL = os.environ.get("SUPABASE_URL")
    SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY") or os.environ.get("SUPABASE_ANON_KEY")
    
    if not SUPABASE_URL or not SUPABASE_KEY:
        dprint(f"[UPLOAD_INTERMEDIATE] Supabase not configured, returning local path")
        return str(local_file_path.resolve())
    
    if not local_file_path.exists():
        dprint(f"[UPLOAD_INTERMEDIATE] File not found: {local_file_path}")
        return None
    
    try:
        headers = {
            "Authorization": f"Bearer {SUPABASE_KEY}",
            "apikey": SUPABASE_KEY,
            "Content-Type": "application/json"
        }
        
        # Step 1: Get signed upload URL - WITH RETRY
        generate_url_edge = f"{SUPABASE_URL.rstrip('/')}/functions/v1/generate-upload-url"
        content_type = mimetypes.guess_type(str(local_file_path))[0] or 'application/octet-stream'
        
        upload_url_resp = None
        for attempt in range(MAX_RETRIES):
            try:
                upload_url_resp = httpx.post(
                    generate_url_edge,
                    headers=headers,
                    json={
                        "task_id": task_id,
                        "filename": filename,
                        "content_type": content_type
                    },
                    timeout=30 + (attempt * 15)
                )
                
                if upload_url_resp.status_code == 200:
                    break
                elif upload_url_resp.status_code in RETRYABLE_STATUS_CODES and attempt < MAX_RETRIES - 1:
                    wait_time = 2 ** attempt
                    dprint(f"[UPLOAD_INTERMEDIATE] generate-upload-url got {upload_url_resp.status_code}, retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    break  # Non-retryable error
                    
            except (httpx.TimeoutException, httpx.RequestError) as e:
                if attempt < MAX_RETRIES - 1:
                    wait_time = 2 ** attempt
                    dprint(f"[UPLOAD_INTERMEDIATE] generate-upload-url error, retrying in {wait_time}s: {e}")
                    time.sleep(wait_time)
                    continue
                else:
                    dprint(f"[UPLOAD_INTERMEDIATE] [EDGE_FAIL:generate-upload-url:NETWORK] {e}")
                    return None
        
        if not upload_url_resp or upload_url_resp.status_code != 200:
            error_text = upload_url_resp.text[:200] if upload_url_resp else "No response"
            error_code = upload_url_resp.status_code if upload_url_resp else "N/A"
            dprint(f"[UPLOAD_INTERMEDIATE] [EDGE_FAIL:generate-upload-url:HTTP_{error_code}] {error_text}")
            return None
        
        upload_data = upload_url_resp.json()
        upload_url = upload_data.get("upload_url")
        storage_path = upload_data.get("storage_path")
        
        if not upload_url:
            dprint(f"[UPLOAD_INTERMEDIATE] No upload_url in response")
            return None
        
        # Step 2: Upload file via signed URL - WITH RETRY
        # IMPORTANT: do NOT read entire file into memory (can be large).
        file_size_mb = local_file_path.stat().st_size / 1024 / 1024
        dprint(f"[UPLOAD_INTERMEDIATE] Uploading {local_file_path.name} ({file_size_mb:.1f} MB)")
        
        put_resp = None
        for attempt in range(MAX_RETRIES):
            try:
                with open(local_file_path, 'rb') as f:
                    put_resp = httpx.put(
                        upload_url,
                        headers={"Content-Type": content_type},
                        content=f,
                        timeout=300 + (attempt * 60)  # 5 min base, +1 min per retry
                    )
                
                if put_resp.status_code in [200, 201]:
                    break
                elif put_resp.status_code in RETRYABLE_STATUS_CODES and attempt < MAX_RETRIES - 1:
                    wait_time = 2 ** attempt
                    dprint(f"[UPLOAD_INTERMEDIATE] storage-upload got {put_resp.status_code}, retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    break  # Non-retryable error
                    
            except (httpx.TimeoutException, httpx.RequestError) as e:
                if attempt < MAX_RETRIES - 1:
                    wait_time = 2 ** attempt
                    dprint(f"[UPLOAD_INTERMEDIATE] storage-upload error, retrying in {wait_time}s: {e}")
                    time.sleep(wait_time)
                    continue
                else:
                    dprint(f"[UPLOAD_INTERMEDIATE] [EDGE_FAIL:storage-upload:NETWORK] {e}")
                    return None
        
        if not put_resp or put_resp.status_code not in [200, 201]:
            error_text = put_resp.text[:200] if put_resp else "No response"
            error_code = put_resp.status_code if put_resp else "N/A"
            dprint(f"[UPLOAD_INTERMEDIATE] [EDGE_FAIL:storage-upload:HTTP_{error_code}] {error_text}")
            return None
        
        # Construct public URL
        public_url = f"{SUPABASE_URL}/storage/v1/object/public/image_uploads/{storage_path}"
        dprint(f"[UPLOAD_INTERMEDIATE] ‚úÖ Uploaded to: {public_url}")
        
        return public_url
        
    except Exception as e:
        dprint(f"[UPLOAD_INTERMEDIATE] Exception: {e}")
        traceback.print_exc()
        return None


def create_mask_video_from_inactive_indices(
    total_frames: int,
    resolution_wh: tuple[int, int], 
    inactive_frame_indices: set[int] | list[int],
    output_path: Path | str,
    fps: int = 16,
    task_id_for_logging: str = "unknown",
    *, dprint = print
) -> Path | None:
    """
    Create a mask video where:
    - Black frames (0) = inactive/keep original - don't edit these frames
    - White frames (255) = active/generate - model should generate these frames
    
    Args:
        total_frames: Total number of frames in the video
        resolution_wh: (width, height) tuple for video resolution
        inactive_frame_indices: Set or list of frame indices that should be black (inactive)
        output_path: Where to save the mask video
        fps: Frames per second for the output video
        task_id_for_logging: Task ID for debug logging
        dprint: Print function for logging
        
    Returns:
        Path to created mask video, or None if creation failed
    """
    try:
        if total_frames <= 0:
            dprint(f"[WARNING] Task {task_id_for_logging}: Cannot create mask video with {total_frames} frames")
            return None
            
        h, w = resolution_wh[1], resolution_wh[0]  # height, width
        inactive_set = set(inactive_frame_indices) if not isinstance(inactive_frame_indices, set) else inactive_frame_indices
        
        dprint(f"Task {task_id_for_logging}: Creating mask video - total_frames={total_frames}, "
               f"inactive_indices={sorted(list(inactive_set))[:10]}{'...' if len(inactive_set) > 10 else ''}")
        
        # Create mask frames: 0 (black) for inactive, 255 (white) for active
        mask_frames_buf: list[np.ndarray] = [
            np.full((h, w, 3), 0 if idx in inactive_set else 255, dtype=np.uint8)
            for idx in range(total_frames)
        ]
        
        created_mask_video = create_video_from_frames_list(
            mask_frames_buf, 
            Path(output_path), 
            fps, 
            resolution_wh
        )
        
        if created_mask_video and created_mask_video.exists():
            dprint(f"Task {task_id_for_logging}: Mask video created successfully at {created_mask_video}")
            return created_mask_video
        else:
            dprint(f"[WARNING] Task {task_id_for_logging}: Failed to create mask video at {output_path}")
            return None
            
    except Exception as e:
        dprint(f"[ERROR] Task {task_id_for_logging}: Mask video creation failed: {e}")
        return None

def create_simple_first_frame_mask_video(
    total_frames: int,
    resolution_wh: tuple[int, int],
    output_path: Path | str, 
    fps: int = 16,
    task_id_for_logging: str = "unknown",
    *, dprint = print
) -> Path | None:
    """
    Convenience function to create a mask video where only the first frame is inactive (black).
    This is useful for workflows where you want to keep the first frame unchanged
    and generate the rest.
    
    Returns:
        Path to created mask video, or None if creation failed
    """
    return create_mask_video_from_inactive_indices(
        total_frames=total_frames,
        resolution_wh=resolution_wh,
        inactive_frame_indices={0},  # Only first frame is inactive
        output_path=output_path,
        fps=fps,
        task_id_for_logging=task_id_for_logging,
        dprint=dprint
    )

def wait_for_file_stable(path: Path | str, checks: int = 3, interval: float = 1.0, *, dprint=print) -> bool:
    """Return True when the file size stays constant for a few consecutive checks.
    Useful to make sure long-running encoders have finished writing before we
    copy/move the file.
    """
    p = Path(path)
    if not p.exists():
        return False
    last_size = p.stat().st_size
    stable_count = 0
    for _ in range(checks):
        time.sleep(interval)
        new_size = p.stat().st_size
        if new_size == last_size and new_size > 0:
            stable_count += 1
            if stable_count >= checks - 1:
                return True
        else:
            stable_count = 0
            last_size = new_size
    return False

def report_orchestrator_failure(task_params_dict: dict, error_msg: str, dprint: callable = print) -> None:
    """Update the parent orchestrator task to FAILED when a sub-task encounters a fatal error.

    Args:
        task_params_dict: The params payload of the *current* sub-task.
            It is expected to contain a reference to the orchestrator via one
            of the standard keys (e.g. ``orchestrator_task_id_ref``).
        error_msg: Human-readable message describing the failure.
        dprint: Debug print helper (typically passed from the caller).
    """
    # Defer import to avoid potential circular dependencies at module import time
    try:
        from source import db_operations as db_ops  # type: ignore
    except Exception as e:  # pragma: no cover
        dprint(f"[report_orchestrator_failure] Could not import db_operations: {e}")
        return

    orchestrator_id = None
    # Common payload keys that may reference the orchestrator task
    for key in (
        "orchestrator_task_id_ref",
        "orchestrator_task_id",
        "parent_orchestrator_task_id",
        "orchestrator_id",
    ):
        orchestrator_id = task_params_dict.get(key)
        if orchestrator_id:
            break

    if not orchestrator_id:
        dprint(
            f"[report_orchestrator_failure] No orchestrator reference found in payload. Message: {error_msg}"
        )
        return

    # Truncate very long messages to avoid DB column overflow
    truncated_msg = error_msg[:500]

    try:
        db_ops.update_task_status(
            orchestrator_id,
            db_ops.STATUS_FAILED,
            truncated_msg,
        )
        dprint(
            f"[report_orchestrator_failure] Marked orchestrator task {orchestrator_id} as FAILED with message: {truncated_msg}"
        )
    except Exception as e_update:  # pragma: no cover
        dprint(
            f"[report_orchestrator_failure] Failed to update orchestrator status for {orchestrator_id}: {e_update}"
        )

def validate_lora_file(file_path: Path, filename: str) -> tuple[bool, str]:
    """
    Validates a LoRA file for size and format integrity.
    
    Returns:
        (is_valid, error_message)
    """
    if not file_path.exists():
        return False, f"File does not exist: {file_path}"
    
    file_size = file_path.stat().st_size
    
    # Known LoRA size ranges (in bytes)
    # These are based on common LoRA architectures and rank sizes
    LORA_SIZE_RANGES = {
        # Very small LoRAs (rank 4-8)
        'tiny': (1_000_000, 50_000_000),      # 1MB - 50MB
        # Standard LoRAs (rank 16-32) 
        'standard': (50_000_000, 500_000_000),  # 50MB - 500MB
        # Large LoRAs (rank 64+) or full model fine-tunes
        'large': (500_000_000, 5_000_000_000), # 500MB - 5GB
        # Extremely large (full model weights)
        'xlarge': (5_000_000_000, 50_000_000_000)  # 5GB - 50GB
    }
    
    # Check if file size is within any reasonable range
    in_valid_range = any(
        min_size <= file_size <= max_size 
        for min_size, max_size in LORA_SIZE_RANGES.values()
    )
    
    if not in_valid_range:
        if file_size < 1_000_000:  # Less than 1MB
            return False, f"File too small ({file_size:,} bytes) - likely corrupted or incomplete download"
        elif file_size > 50_000_000_000:  # More than 50GB
            return False, f"File too large ({file_size:,} bytes) - likely not a LoRA file"
    
    # For safetensors files, try to open and inspect
    if filename.endswith('.safetensors'):
        try:
            import safetensors.torch as st
            with st.safe_open(file_path, framework="pt") as f:
                # Get metadata to verify it's actually a LoRA
                metadata = f.metadata()
                keys = list(f.keys())
                
                # LoRAs typically have keys like "lora_down.weight", "lora_up.weight", etc.
                lora_indicators = ['lora_down', 'lora_up', 'lora.down', 'lora.up', 'lora_A', 'lora_B']
                has_lora_keys = any(indicator in key for key in keys for indicator in lora_indicators)
                
                if not has_lora_keys and len(keys) > 100:
                    # Might be a full model checkpoint rather than a LoRA
                    print(f"[WARNING] {filename} appears to be a full model checkpoint ({len(keys)} tensors) rather than a LoRA")
                elif not has_lora_keys:
                    print(f"[WARNING] {filename} doesn't appear to contain LoRA weights (no lora_* keys found)")
                
                # Check for reasonable number of parameters
                if len(keys) == 0:
                    return False, "Safetensors file contains no tensors"
                elif len(keys) > 10000:
                    print(f"[WARNING] {filename} contains many tensors ({len(keys)}) - might be a full model")
                    
        except ImportError:
            print(f"[WARNING] safetensors not available for detailed validation of {filename}")
        except Exception as e:
            return False, f"Safetensors file appears corrupted: {e}"
    
    # Additional checks for common corruption patterns
    if file_size == 0:
        return False, "File is empty"
    
    # For binary files, check they don't start with common error HTML patterns
    try:
        with open(file_path, 'rb') as f:
            first_bytes = f.read(1024)
            if first_bytes.startswith(b'<!DOCTYPE html') or first_bytes.startswith(b'<html'):
                return False, "File appears to be an HTML error page rather than a LoRA file"
    except Exception:
        pass
    
    return True, f"File validated successfully ({file_size:,} bytes)"

def check_loras_in_directory(lora_dir: Path | str, fix_issues: bool = False) -> dict:
    """
    Checks all LoRA files in a directory for integrity issues.
    
    Args:
        lora_dir: Directory containing LoRA files
        fix_issues: If True, removes corrupted files
        
    Returns:
        Dictionary with validation results
    """
    lora_dir = Path(lora_dir)
    if not lora_dir.exists():
        return {"error": f"Directory does not exist: {lora_dir}"}
    
    results = {
        "total_files": 0,
        "valid_files": 0,
        "invalid_files": 0,
        "issues": [],
        "summary": []
    }
    
    # Look for LoRA-like files
    lora_extensions = ['.safetensors', '.bin', '.pt', '.pth']
    lora_files = []
    
    for ext in lora_extensions:
        lora_files.extend(lora_dir.glob(f"*{ext}"))
        lora_files.extend(lora_dir.glob(f"**/*{ext}"))  # Include subdirectories
    
    # Filter to likely LoRA files
    lora_files = [f for f in lora_files if 'lora' in f.name.lower() or f.suffix == '.safetensors']
    
    results["total_files"] = len(lora_files)
    
    for lora_file in lora_files:
        is_valid, validation_msg = validate_lora_file(lora_file, lora_file.name)
        
        if is_valid:
            results["valid_files"] += 1
            results["summary"].append(f"‚úì {lora_file.name}: {validation_msg}")
        else:
            results["invalid_files"] += 1
            issue_msg = f"‚úó {lora_file.name}: {validation_msg}"
            results["issues"].append(issue_msg)
            results["summary"].append(issue_msg)
            
            if fix_issues:
                try:
                    lora_file.unlink()
                    results["summary"].append(f"  ‚Üí Removed corrupted file: {lora_file.name}")
                except Exception as e:
                    results["summary"].append(f"  ‚Üí Failed to remove {lora_file.name}: {e}")
    
    return results

# --------------------------------------------------------------------------------------------------
